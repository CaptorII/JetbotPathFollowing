{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 2: NVIDIA Jetbot Path Following and Pest Detection\n",
    "### Intro\n",
    "This notebook is to be the record of completion for Assessment 2: Real-World Application of Machine Learning and Computer Vision.\n",
    "### Scenario\n",
    "Building on the previous pest identification project, program a bot that can both follow a set course and respond when specific pests are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-setup\n",
    "%pip install -Uqq ipywidgets\n",
    "%pip install -Uqq torch torchvision torchaudio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Following\n",
    "#### Data Collection\n",
    "Setting up the JetBot to follow paths was done in two stages. The following section is the code and widgets I ran on the JetBot to gather the images required for path following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "# Imports for data collection\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    os.makedirs(DATASET_DIR)\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']        \n",
    "        uuid = 'xy_%03d_%03d_%s' % (x, y, uuid1())\n",
    "        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(camera_widget.value)\n",
    "        \n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block stops the camera once data collection is done, then saves the images to a zip file so it can be exported to a more powerful machine to train and refine the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "camera.stop()\n",
    "!zip -r -q path_following_dataset.zip {DATASET_DIR}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Path Following\n",
    "The following code was used to train the model based on the images collected, to teach the bot how to follow the path set out. I used resnet18 for the path following model due to its ubiquity and ease of use, with 15 training epochs for quick and consistent results. This model was trained on CPU rather than the faster GPU, so reducing the epochs substantially increased my ability to make updates to the model and rebuild within a reasonable timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first to unzip dataset\n",
    "!unzip -q path_following_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for training model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_x(path, width):\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        # apply transforms to some images to make the dataset more resilient\n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_xy') # path following training images dataset\n",
    "\n",
    "test_percent = 0.1 # use 10% of images for verification\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "model = models.resnet18(pretrained=True) # using resnet18\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "NUM_EPOCHS = 15 # uses 15 epochs to refine model\n",
    "BEST_MODEL_PATH = 'path_following.pth'\n",
    "best_loss = 1e9\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('Training loss: %f, Testing loss: %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH) # export the best model as a .pth file\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Pest Detection\n",
    "#### Combining Path Following and Bug Identification\n",
    "To combine both models, I set up two folders, one labeled 'free' with images collected when training the path following, and one labeled 'blocked' with images of insects from the bug finding project. The new model combines both of these datasets to allow the JetBot to follow the path and check for insects, and will stop when it detects an insect matching one it was trained on. The following code is importing the previously trained models to set up this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing models for path following and bug detection\n",
    "path_model = torchvision.models.resnet18(pretrained=False)\n",
    "path_model.fc = torch.nn.Linear(512, 2)\n",
    "path_model.load_state_dict(torch.load('path_following.pth'))\n",
    "\n",
    "bug_model = torchvision.models.alexnet(pretrained=False)\n",
    "bug_model.classifier[6] = torch.nn.Linear(bug_model.classifier[6].in_features, 2)\n",
    "bug_model.load_state_dict(torch.load('bug_finder.pth'))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "path_model = path_model.to(device)\n",
    "bug_model = bug_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following segment of code was run on the JetBot to refine the parameters used by testing impacts of changing them in real time. The first section of code is setting up the widgets to control the JetBot and update its parameters on the fly, and the second attaches the controls to the JetBot itself. This allowed me to set optimal parameters to get the best results from the models I had.\n",
    "\n",
    "I found that using a speed of 0.1 worked best as the JetBot moved along slow enough that it could analyse its path well and react to changes efficiently. The insect identification did not work well, so I found using a 'blocked' level of anything less than 25% meant the bot would never trigger that condition and never stop. This meant it did occasionally have false positives, like chair wheels being identified as bugs, but allowed the model to register actual positives also. Using a lengthy stop time (>20 frames) made it easier to identify when the insect identification model had worked, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "camera = Camera()\n",
    "image_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "robot = Robot()\n",
    "# path following sliders\n",
    "speed_control_slider = ipywidgets.FloatSlider(min=0.0, max=0.2, step=0.05, description='Speed control')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.04, description='Steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='Steering dgain')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='Steering bias')\n",
    "# bug identifying sliders\n",
    "blocked_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, orientation='horizontal', description='Blocked level')\n",
    "stopduration_slider= ipywidgets.IntSlider(min=20, max=1000, step=20, value=40, description='Time to stop') \n",
    "blocked_threshold= ipywidgets.FloatSlider(min=0, max=1.0, step=0.05, value=0.25, description='Blocked threshold')\n",
    "display(speed_control_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "display(image_widget)\n",
    "display(ipywidgets.HBox([blocked_slider, blocked_threshold, stopduration_slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "count_stops = 0\n",
    "go_on = 1\n",
    "stop_time = 40 # The number of frames to remain stopped\n",
    "x = 0.0\n",
    "y = 0.0\n",
    "speed_value = speed_control_slider.value\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "# functions for pre-processing images to be used by bot\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "def preprocess_col(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last, blocked_slider, robot, count_stops, stop_time, go_on, x, y, blocked_threshold\n",
    "    global speed_value, steer_gain, steer_dgain, steer_bias                \n",
    "    steer_gain = steering_gain_slider.value\n",
    "    steer_dgain = steering_dgain_slider.value\n",
    "    steer_bias = steering_bias_slider.value       \n",
    "    image_preproc = preprocess(change['new']).to(device)\n",
    "    image_preproc2 = preprocess_col(change['new']).to(device)\n",
    "    \n",
    "    prob_blocked = float(F.softmax(bug_model(image_preproc2), dim=1).flatten()[0])    \n",
    "    blocked_slider.value = prob_blocked    \n",
    "    stop_time=stopduration_slider.value\n",
    "    \n",
    "    if go_on == 1:    \n",
    "        if prob_blocked > blocked_threshold.value: # if insect detected by camera\n",
    "            count_stops += 1\n",
    "            go_on = 2\n",
    "        else: # if no insect found, follow path\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "            xy = path_model(image_preproc2).detach().float().cpu().numpy().flatten()        \n",
    "            x = xy[0]            \n",
    "            y = (0.5 - xy[1]) / 2.0\n",
    "            speed_value = speed_control_slider.value\n",
    "    else:\n",
    "        count_stops += 1\n",
    "        if count_stops < stop_time:\n",
    "            x = 0.0\n",
    "            y = 0.0\n",
    "            speed_value = 0 \n",
    "        else:\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "                \n",
    "    angle = math.atan2(x, y)        \n",
    "    pid = angle * steer_gain + (angle - angle_last) * steer_dgain\n",
    "    steer_val = pid + steer_bias \n",
    "    angle_last = angle\n",
    "    robot.left_motor.value = max(min(speed_value + steer_val, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_value - steer_val, 1.0), 0.0) \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bot using model - run when previous cells are done and you are ready to start the bot\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop bot - run when testing is complete\n",
    "camera.unobserve(execute, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "robot.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Demonstration:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls src=\"BugPath.mp4\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Evaluation:\n",
    "The way this model was trained - on a select number of stock images of insects sourced from the internet - very likely hindered its performance in testing. This model was quite bad at identifying images of insects unless they were very close to its camera (taking up most of the screen), or if the bot could not see the line it was supposed to be following. The line-following portion worked very well, the bot was able to follow the line fairly reliably, and was usually able to find its way back to the line if it did drift off-course. This may be partly because the model had many images to reference with path following, and only a fraction of that number of images of the type of insect we used to test its effectiveness. \n",
    "\n",
    "\n",
    "With the JetBot I was using, there was no cooling fan or active cooling on the bot, so I was only able to test with it for short durations. The addition of a cooling fan would allow more reliable testing, and likely also improve inference speed. Replacing the JetBot with more powerful hardware would also be likely to increase inference speed.\n",
    "\n",
    "\n",
    "For future improvements, a greater number of insect images in a greater variety of settings and distances would improve the pest detection accuracy. Better hardware and better cooling would improve inference speed. A broader model trained not only on a handful of insect types but a range of different insects would be more generalizable and could be used in more scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
