{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment 2: Path Following and Collision Avoiding\n",
    "### Intro\n",
    "This notebook is to be the record of completion for Assessment 2: Programming JetBots.\n",
    "### Scenario\n",
    "Building on the previous pest identification project, program a bot that can both follow a set course and respond when specific pests are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-setup\n",
    "# %pip install -Uqq ipywidgets\n",
    "# %pip install -Uqq torch\n",
    "# %pip install -Uqq cv2\n",
    "# %pip install -Uqq jetbot\n",
    "# %pip install -Uqq numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Following\n",
    "#### Data Collection\n",
    "The following section is the code and widgets I ran on the JetBot to gather the images required to train the bot in path following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "# Imports for data collection\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    os.makedirs(DATASET_DIR)\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "# manually update counts at initialization\n",
    "count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # save to disk\n",
    "        #dataset.save_entry(category_widget.value, camera.value, x, y)\n",
    "        uuid = 'xy_%03d_%03d_%s' % (x, y, uuid1())\n",
    "        image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(camera_widget.value)\n",
    "        \n",
    "        # display saved snapshot\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block stops the camera once data collection is done, then saves the images to a zip file so it can be exported to a more powerful machine to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "camera.stop()\n",
    "\n",
    "!zip -r -q path_following_dataset.zip {DATASET_DIR}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Path Following\n",
    "The following code was used to train the model based on the images collected, to teach the bot how to follow the path set out. I used resnet18 for the path following model due to its ubiquity and ease of use, for quick and consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run first to unzip dataset\n",
    "!unzip -q path_following_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for training model\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import PIL.Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_x(path, width):\n",
    "    \"\"\"Gets the x value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[1])) - width/2) / (width/2)\n",
    "\n",
    "def get_y(path, height):\n",
    "    \"\"\"Gets the y value from the image filename\"\"\"\n",
    "    return (float(int(path.split(\"_\")[2])) - height/2) / (height/2)\n",
    "\n",
    "class XYDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, directory, random_hflips=False):\n",
    "        self.directory = directory\n",
    "        self.random_hflips = random_hflips\n",
    "        self.image_paths = glob.glob(os.path.join(self.directory, '*.jpg'))\n",
    "        self.color_jitter = transforms.ColorJitter(0.3, 0.3, 0.3, 0.3)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        image = PIL.Image.open(image_path)\n",
    "        width, height = image.size\n",
    "        x = float(get_x(os.path.basename(image_path), width))\n",
    "        y = float(get_y(os.path.basename(image_path), height))\n",
    "      \n",
    "        if float(np.random.rand(1)) > 0.5:\n",
    "            image = transforms.functional.hflip(image)\n",
    "            x = -x\n",
    "        \n",
    "        image = self.color_jitter(image)\n",
    "        image = transforms.functional.resize(image, (224, 224))\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image = image.numpy()[::-1].copy()\n",
    "        image = torch.from_numpy(image)\n",
    "        image = transforms.functional.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, torch.tensor([x, y]).float()\n",
    "    \n",
    "dataset = XYDataset('dataset_xy', random_hflips=False)\n",
    "\n",
    "test_percent = 0.1\n",
    "num_test = int(test_percent * len(dataset))\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - num_test, num_test])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(512, 2)\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "\n",
    "NUM_EPOCHS = 40\n",
    "BEST_MODEL_PATH = 'path_following.pth'\n",
    "best_loss = 1e9\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in iter(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        train_loss += float(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    for images, labels in iter(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = F.mse_loss(outputs, labels)\n",
    "        test_loss += float(loss)\n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print('%f, %f' % (train_loss, test_loss))\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        best_loss = test_loss\n",
    "print(\"finished\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Pest Detection\n",
    "#### Combining Path Following and Bug Identification\n",
    "To combine both models, I set up two folders, one labeled 'free' with images collected when training the path following, and one labeled 'blocked' with images of insects from the bug finding project. The new model combines both of these datasets to allow the JetBot to follow the path and check for insects, and will stop when it detects an insect matching one it was trained on. The following code is importing the previously trained models to set up this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing models for path following and bug detection\n",
    "path_model = torchvision.models.resnet18(pretrained=False)\n",
    "path_model.fc = torch.nn.Linear(512, 2)\n",
    "path_model.load_state_dict(torch.load('path_following.pth'))\n",
    "\n",
    "bug_model = torchvision.models.alexnet(pretrained=False)\n",
    "bug_model.classifier[6] = torch.nn.Linear(bug_model.classifier[6].in_features, 2)\n",
    "bug_model.load_state_dict(torch.load('bug_finder.pth')) # how to export as .pth?\n",
    "\n",
    "device = torch.device('cuda')\n",
    "path_model = path_model.to(device)\n",
    "bug_model = bug_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following segment of code was run on the JetBot to refine the parameters used by testing impacts of changing them in real time. This allowed me to set optimal parameters for the next stage of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN ON JETBOT ##\n",
    "camera = Camera()\n",
    "image_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "robot = Robot()\n",
    "#Road Following sliders\n",
    "speed_control_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='Speed control')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.04, description='Steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='Steering dgain')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='Steering bias')\n",
    "display(speed_control_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "#Collision Avoidance sliders\n",
    "blocked_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, orientation='horizontal', description='Blocked')\n",
    "stopduration_slider= ipywidgets.IntSlider(min=1, max=1000, step=1, value=10, description='Time to stop') \n",
    "blocked_threshold= ipywidgets.FloatSlider(min=0, max=1.0, step=0.01, value=0.8, description='Blocked threshold')\n",
    "display(image_widget)\n",
    "display(ipywidgets.HBox([blocked_slider, blocked_threshold, stopduration_slider]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "count_stops = 0\n",
    "go_on = 1\n",
    "stop_time = 10 # The number of frames to remain stopped\n",
    "x = 0.0\n",
    "y = 0.0\n",
    "speed_value = speed_control_slider.value\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last, blocked_slider, robot, count_stops, stop_time, go_on, x, y, blocked_threshold\n",
    "    global speed_value, steer_gain, steer_dgain, steer_bias\n",
    "                \n",
    "    steer_gain = steering_gain_slider.value\n",
    "    steer_dgain = steering_dgain_slider.value\n",
    "    steer_bias = steering_bias_slider.value\n",
    "       \n",
    "    image_preproc = preprocess(change['new']).to(device)\n",
    "    image_preproc2 = preprocess_col(change['new']).to(device)\n",
    "     \n",
    "    # Bug Detection model:\n",
    "    \n",
    "    prob_blocked = float(F.softmax(bug_model(image_preproc2), dim=1).flatten()[0])\n",
    "    \n",
    "    blocked_slider.value = prob_blocked    \n",
    "    stop_time=stopduration_slider.value\n",
    "    \n",
    "    if go_on == 1:    \n",
    "        if prob_blocked > blocked_threshold.value: # threshold should be above 0.5\n",
    "            count_stops += 1\n",
    "            go_on = 2\n",
    "        else:\n",
    "            #start of road following detection\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "            xy = path_model(image_preproc2).detach().float().cpu().numpy().flatten()        \n",
    "            x = xy[0]            \n",
    "            y = (0.5 - xy[1]) / 2.0\n",
    "            speed_value = speed_control_slider.value\n",
    "    else:\n",
    "        count_stops += 1\n",
    "        if count_stops < stop_time:\n",
    "            x = 0.0 #set x steering to zero\n",
    "            y = 0.0 #set y steering to zero\n",
    "            speed_value = 0 # set speed to zero (can set to turn as well)\n",
    "        else:\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "                \n",
    "    angle = math.atan2(x, y)        \n",
    "    pid = angle * steer_gain + (angle - angle_last) * steer_dgain\n",
    "    steer_val = pid + steer_bias \n",
    "    angle_last = angle\n",
    "    robot.left_motor.value = max(min(speed_value + steer_val, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_value - steer_val, 1.0), 0.0) \n",
    "\n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run bot using model\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop bot\n",
    "camera.unobserve(execute, names='value')\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "robot.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Demonstration\n",
    "**** Insert video here ****\n",
    "\n",
    "\n",
    "You must provide a short (less than 1-minute) video demonstrating the Jetbot in action (using Xbox game bar recording or simlilar). The video should showcase the path following and pest detection aspects."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Evaluation\n",
    "You must critically evaluate the accuracy and effectiveness of your trained model for pest detection during infernece.\n",
    "You must offer insightful comments on areas where improvements can be made, such as model accuracy, inference speed, and generalizability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Road Following**   \n",
    "- Upload the \"*best_steering_model_xy_trt.pth*\" model file obtained from the \"live_demo_build_trt.ipynb\" into this notebooks's directory. Once that's finished there should be a file named ``best_steering_model_xy_trt.pth`` in this notebook's directory.    \n",
    "\n",
    "### **Collision Avoidance**             \n",
    "- Upload the \"*best_model_trt.pth*\" model file obtained from the \"live_demo_resnet18_build_trt.ipnb\" into this notebooks's directory. Once that's finished there should be a file named ``best_model_trt.pth`` in this notebook's directory.   \n",
    "> **Note**: Collision Avoidance ``blocked`` class should include images of cars or any obstacle which you would expect to see on the road (It's good to start with one object first) meanwhile the ``free`` class should include background images of the road following track. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TRT optimized models by executing the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model = torchvision.models.resnet18(pretrained=False)\n",
    "path_model.fc = torch.nn.Linear(512, 2)\n",
    "path_model.load_state_dict(torch.load('best_steering_model_xy.pth')) # well trained road following model\n",
    "\n",
    "bug_model = torchvision.models.alexnet(pretrained=False)\n",
    "bug_model.classifier[6] = torch.nn.Linear(bug_model.classifier[6].in_features, 2)\n",
    "bug_model.load_state_dict(torch.load('best_model.pth')) # well trained collision avoidance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "path_model = path_model.to(device)\n",
    "bug_model = bug_model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our models, but there's a slight issue. The format that we trained our models on doesn't exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "normalize = torchvision.transforms.Normalize(mean, std)\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "def preprocess_col(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera. You should be pretty familiar with this by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = Camera()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ipywidgets.Image()\n",
    "\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define some sliders to control the JetBot\n",
    "> **Note**: We have initialized the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\n",
    "\n",
    "1. Speed Control slider: To start your JetBot increase ``speed_control_slider`` \n",
    "2. Steering Gain slider: If you see your JetBot is woblling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "3. Steering Bias slider: If you see your JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "\n",
    "> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior.\n",
    "\n",
    "4. Blocked slider: Display the probability in which there is an obstacle in the front of the Jetbot using the collision avoidance model\n",
    "5. Time for stop slider: To manually set the time for which the jetbot should remain stopped after an object has been removed\n",
    "6. Blocked threshold slider: To manually set the blocked threshold to stop the Jetbot after an object has been detected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Road Following sliders\n",
    "speed_control_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed control')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.04, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "display(speed_control_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)\n",
    "\n",
    "#Collision Avoidance sliders\n",
    "blocked_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, orientation='horizontal', description='blocked')\n",
    "stopduration_slider= ipywidgets.IntSlider(min=1, max=1000, step=1, value=10, description='time for stop') \n",
    "blocked_threshold= ipywidgets.FloatSlider(min=0, max=1.0, step=0.01, value=0.8, description='blocked threshold')\n",
    "\n",
    "display(image_widget)\n",
    "\n",
    "display(ipywidgets.HBox([blocked_slider, blocked_threshold, stopduration_slider]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network models for Road following and Collision Avoidance\n",
    "3. Check an if statements which would allow the Jetbot to perform road following and stop whenever an obstacle has been detected \n",
    "4. Compute the approximate steering value\n",
    "5. Control the motors using proportional / derivative control (PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "count_stops = 0\n",
    "go_on = 1\n",
    "stop_time = 10 # The number of frames to remain stopped\n",
    "x = 0.0\n",
    "y = 0.0\n",
    "speed_value = speed_control_slider.value\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last, blocked_slider, robot, count_stops, stop_time, go_on, x, y, blocked_threshold\n",
    "    global speed_value, steer_gain, steer_dgain, steer_bias\n",
    "                \n",
    "    steer_gain = steering_gain_slider.value\n",
    "    steer_dgain = steering_dgain_slider.value\n",
    "    steer_bias = steering_bias_slider.value\n",
    "       \n",
    "    image_preproc = preprocess(change['new']).to(device)\n",
    "    image_preproc2 = preprocess_col(change['new']).to(device)\n",
    "     \n",
    "    #Collision Avoidance model:\n",
    "    \n",
    "    prob_blocked = float(F.softmax(bug_model(image_preproc2), dim=1).flatten()[0])\n",
    "    \n",
    "    blocked_slider.value = prob_blocked    \n",
    "    stop_time=stopduration_slider.value\n",
    "    \n",
    "    if go_on == 1:    \n",
    "        if prob_blocked > blocked_threshold.value: # threshold should be above 0.5\n",
    "            count_stops += 1\n",
    "            go_on = 2\n",
    "        else:\n",
    "            #start of road following detection\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "            xy = path_model(image_preproc2).detach().float().cpu().numpy().flatten()        \n",
    "            x = xy[0]            \n",
    "            y = (0.5 - xy[1]) / 2.0\n",
    "            speed_value = speed_control_slider.value\n",
    "    else:\n",
    "        count_stops += 1\n",
    "        if count_stops < stop_time:\n",
    "            x = 0.0 #set x steering to zero\n",
    "            y = 0.0 #set y steering to zero\n",
    "            speed_value = 0 # set speed to zero (can set to turn as well)\n",
    "        else:\n",
    "            go_on = 1\n",
    "            count_stops = 0\n",
    "            \n",
    "    \n",
    "    angle = math.atan2(x, y)        \n",
    "    pid = angle * steer_gain + (angle - angle_last) * steer_dgain\n",
    "    steer_val = pid + steer_bias \n",
    "    angle_last = angle\n",
    "    robot.left_motor.value = max(min(speed_value + steer_val, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_value - steer_val, 1.0), 0.0) \n",
    "\n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "We accomplish that with the observe function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower and collision avoider should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow the track and avoid collisions effectively.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Jetbot:\n",
    "\n",
    "You are required to use the NVIDIA Jetbot to train it to follow a predefined path within a specified environment, in this case the classroom following the pink tape.\n",
    "You must document the entire training process, including code, model selection, and training parameters, in a Jupyter notebook.\n",
    "The Jupyter notebook should include comments explaining the rationale behind their choices and any issues they encountered.\n",
    "### Inference for Pest Detection:\n",
    "\n",
    "Using the model you have created in Assessment 1 (or a similar one), you must implement inference to detect pests.\n",
    "You need to demonstrate how the Jetbot can identify and respond to pests in real-time during its path following.\n",
    "The Jupyter notebook should contain code for inference and comments explaining the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
